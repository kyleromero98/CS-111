NAME: Kyle Romero
EMAIL: kyleromero98@gmail.com
ID: 204747283

Project 2B - Lock Granularity and Performance

Contents - Note: Some of these statements were taken directly from the project specification
SortedList.h - the C header file for a doubly linked list
SortedList.c - the C implementation file for a doubly linked list
lab2_list.c - the C source file that implements and tests the shared data structure list, produces command line output, and test results
Makefile - the Makefile with targets for building, doing tests, making graphs, profiling, and building for distribution
lab2b_list.csv - the output data for my tests with the list portion of this lab
profile.out - the profiling report that shows where time was spent in the un-partitioned spin lock implementation
lab2b_1.png - throughput vs. number of threads for mutex and spin-lock synchronized list operations.
lab2b_2.png - mean time per mutex wait and mean time per operation for mutex-synchronized list operations.
lab2b_3.png - successful iterations vs. threads for each synchronization method.
lab2b_4.png - throughput vs. number of threads for mutex synchronized partitioned lists.
lab2b_5.png - throughput vs. number of threads for spin-lock-synchronized partitioned lists.
README - contains information regarding the contents of the TAR for this project, the answers to the lab questions, and some notes on research/citations
lab2_list.gp - a script which generates the graphs for the list portion of this lab

Questions
2.3.1 - For the 1 and 2 thread cases, I hypothesize that we spend the majority of our cycles performing the list operations since very little
time should be spent checking lock conditions. This is because there is either 1 thread, and so now locking needs to occur, or 2 threads that
will have a small chance of a collision, and thus spinning, inside of the critical section.

I believe that the list operations are much more expensive because spinning does not generally happen very often with few threads and even when
it does it only consists of some kind of simple check. Whereas, inserting/deleting/looking-up requires expensive operations on a linked list that
often traverse many elements before returning.

With high-thread spin-lock tests, I believe that the majority of time is spent waiting for locks. As the number of threads increases, the amount
of time/cycles spent waiting on spin locks will increase because there are more collisons between when certain threads want to access the critical
sections.

With high-thread mutex cases, I believe that the majority of time is spent performing list operations. In special cases where the number of threads is
very large or the length of the list is very small, the majority of time may changce to being spent performing context switches. This would be because
we are constantly switching the context of the running thread due to the sleeping of mutexes.

2.3.2 - The lines of code that contain the spin-lock checks end up being the most expensive. Specifically, lines 71 and 120 in my code.

This operation becomes so expensive with a large number of threads because lock contention occurs more often with more threads. Therefore, a lot of
extra CPU cycles are spent checking these conditions and end up dominating the execution time when there are a lot of threads.

2.3.3 - The average lock wait time increases dramatically with the number of threads because there is more lock contention as the number of threads
increases. Therefore, the average amount of time that any given thread will wait to acquire a lock is going to increase.

Average completion time rises because lock contention has increased as the number of threads increases. The amount of time spent waiting and the amount
of time performing context switches increases too. This accounts for the increase in the average completion time per thread.

The average completion time per operation increases less and is lower than the average lock wait time because the data gathered by the average lock wait
time counters is overlapping. This allows for the counter to accumulate to a higher number.

2.3.4 - The throughput for higher list counts is higher because the more lists there are, the less lock contention occurs. That is why we see the general trend that
the average throughput increases with the number of sublists.

As the number of lists increases, we will approach the theoretical lowest possible average throughput. So, it will continue to increase for a little bit but then
the increase will plateau as lock contention is decreased to almost 0. Therefore, it should for some more small increases but will eventually not increase past a
certain point.

Yes, this seems to be what my graphs suggest. For example, in graph 4 we see that around 8 threads the 8-way partition list has significantly lower throughput than
the single list with 1/8th the number of threads (1 thread).

Sources/Citations
The following were used as examples in writing the code for this project.

Hash Function Source
https://stackoverflow.com/questions/7666509/hash-function-for-string
